# Misc weights
tok_embeddings.weight: torch.Size([32000, 1664])
norm.weight: torch.Size([6656]) # no merge
output.weight: torch.Size([8000, 6656])
rope.freqs: torch.Size([64]) # unused maybe?

# weight per tf layer (0~59; total 60)
layers.*.attention.wq.weight: torch.Size([1664, 6656])
layers.*.attention.wk.weight: torch.Size([1664, 6656])
layers.*.attention.wv.weight: torch.Size([1664, 6656])
layers.*.attention.wo.weight: torch.Size([6656, 1664])
layers.*.feed_forward.w1.weight: torch.Size([4480, 6656])
layers.*.feed_forward.w2.weight: torch.Size([6656, 4480])
layers.*.feed_forward.w3.weight: torch.Size([4480, 6656])
layers.*.attention_norm.weight: torch.Size([6656]) # no merge
layers.*.ffn_norm.weight: torch.Size([6656]) # no merge

{"dim": 6656, "multiple_of": 256, "n_heads": 52, "n_layers": 60, "norm_eps": 1e-06, "vocab_size": -1}

V = 32000
H = 6656
D = 17920 (D / 4 == 4480)
Transformer(
  (tok_embeddings): ParallelEmbedding() <- tok_embeddings.weight
  # MP: [B, S] (idx) x [V, 1664] = [B, S, 1664]
  # original: [B, S] (idx) x [V, 6656] = [B, S, 6656]
  (layers): ModuleList(
    (0-59): 60 x TransformerBlock(
        #h = x + self.attention.forward(self.attention_norm(x), start_pos, freqs_cis, mask)
        #out = h + self.feed_forward.forward(self.ffn_norm(h))
      (attention): Attention(
        (wq): ColumnParallelLinear() <- layers.*.attention.wq.weight (H/4, H)
        (wk): ColumnParallelLinear() <- wk (H/4, H)
        (wv): ColumnParallelLinear() <- wv (H/4, H)
        (wo): RowParallelLinear() <- wo (H, H/4)
      )
      (feed_forward): FeedForward(
        # H -> w1 -> D
        # H -> w3 -> D
        # D (elemwise product) -> w2 -> H
        # self.w2(F.silu(self.w1(x)) -> (D, H) * self.w3(x) -> (D, H)) -> 
        (w1): ColumnParallelLinear()  <- w1(D/4, H)
        (w2): RowParallelLinear() <- w2(H, D/4)
        (w3): ColumnParallelLinear() <-w3(D/4, H)
      )
      (attention_norm): RMSNorm()
      (ffn_norm): RMSNorm()
    )
  )
  (norm): RMSNorm() <- norm.weight
  # original: [B, S, H] -> [B, S, H]
  (output): ColumnParallelLinear() <- output.weight
  # MP: [B, S, H] x [V/4, H] = [B, S, V/4]
  # original: [B, S, H] x [V, H] = [B, S, V]
)

Computation analysis:
full token S
QKV gen: 6SBH^2
QK mul : 2S^2BH
QKV mul: 2S^2BH
linear:  2SBH^2
w1 : BSH * DH -> 2BSHD
w3 : BSH * DH -> 2BSHD
w2 : BSD * DH -> 2BSHD
Outputembed: 2SBHV
Total : L(8SBHH + 4SSBH + 6BSHD)+2SBHV

ctx token C, cont token S

(C+S, S) (C+S, H)

QKV gen: 6SBH^2
QK mul : 2(C+S)SBH
QKV mul: 2(C+S)SBH
linear:  2SBH^2
w1 : BSH * DH -> 2BSHD
w3 : BSH * DH -> 2BSHD
w2 : BSD * DH -> 2BSHD
Outputembed: 2SBHV
Total : L(8SBHH + 4(C+S)SBH + 6BSHD)+2SBHV

S -> X + Y
Original
L(8SBHH + 4SSBH + 6BSHD)+2SBHV
New
L(8XBHH + 4XXBH + 6BXHD)+2XBHV
L(8YBHH + 4SYBH + 6BYHD)+2YBHV
=
L(8SBHH + 4(XX+SY)BH + 6BSHD)+2SBHV
=
L(8SBHH + 4SSBH + 6BSHD)+2SBHV - L(4YXBH)

XX+SY = xx+s(s-x) = xx+ss-sx = ss+(x-s)x = ss - yx
